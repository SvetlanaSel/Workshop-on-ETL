# Лабораторная работа №1. Установка и настройка ETL-инструмента. Создание конвейеров данных. Вариант 13

**Цель работы.** Изучение основных принципов работы с ETL-инструментами на примере Pentaho Data Integration (PDI), настройка среды, создание конвейера обработки данных (фильтрация, очистка, замена значений) и выгрузка результатов в базу данных MySQL.

---
## Шаг 1. Запуск Pentaho Spoon
```bash
./spoon.sh
```
Результат:

<img width="850" height="420" alt="image" src="https://github.com/user-attachments/assets/4aeba747-7f9c-4a07-974d-ef6c59a20450" />


## Шаг 2. Выполнение задания лабораторной работы. Вариант 13

### 1. Настройка импорта данных из источника csv.

Загружаю мой датасет в нужную папку для импорта, далее указываю путь к нему в шаге импорта:

<img width="922" height="732" alt="image" src="https://github.com/user-attachments/assets/91f10439-8c43-4a51-bcd1-82d56ef1dac6" />

Были получены поля, их типы данных и необходимое количество символов для каждого поля.

### 2. Создание таблицы в СУБД.

Перед началом процесса экспорта данных в мою базу СУБД создаю таблицу с соответсвующими полученным столбцам наименованиями и типами данных.

<img width="1681" height="581" alt="image" src="https://github.com/user-attachments/assets/da67b5cc-b20e-48c0-8ecf-0c6e69ca84a1" />

### 3. Корректировка наименований столбцов.

На этом этапе я переименовываю столбцы будущего датасета, чтобы они соответствовали стандартам:

<img width="924" height="601" alt="image" src="https://github.com/user-attachments/assets/6330d7c1-4354-47ab-8a69-cbc47a7057bb" />

### 4. Отбор значений для анализа.

Так как исходный файл содержит некоторые невалидные для анализа столбцы, я не буду включать их в экспорт, и, с помощью системного элемента, я провожу отбор нужных столбов:

<img width="752" height="495" alt="image" src="https://github.com/user-attachments/assets/03f09e5e-9af3-41a4-9110-e882a8b024bf" />

### 5. Замена значений.

В исходном файле есть сведения об отношении людей к посту в социальной сети. Чтобы упростить процесс анализа данных и рассчетов над ними, вместо показателей "Positive", "Negative" и "Neutral" я буду использовать числовые значения, 1 - положительное, 0 - негативное и нейтральное.

<img width="830" height="375" alt="image" src="https://github.com/user-attachments/assets/59e35677-cc3d-458a-9352-9b42c8316372" />

### 6. Настройка экпорта.

Настраиваю параметры экспорта данных, вношу мою БД, сопоставляю столбцы:

<img width="957" height="767" alt="image" src="https://github.com/user-attachments/assets/205f4cc6-f361-41d5-b075-eccfb0042233" />

### 7. Запуск файла.

Запускаю файл с моей схемой:

<img width="1068" height="561" alt="image" src="https://github.com/user-attachments/assets/1e73b3b6-ab5d-4d6b-9387-82409f36ba9f" />

Можно увидеть, что все успешно выполнилось. Далее проверчю, подгрузились ли данные в СУБД:

<img width="1423" height="711" alt="image" src="https://github.com/user-attachments/assets/234c28dd-046c-4717-9586-20541b65de93" />

Да, данные появились в СУБД.

### 8. Проверка с помощью sql - запроса.

Выведу рейтинг каждой платформы:
```
SELECT platform, COUNT(*) as total_posts, 
ROUND(AVG(likes), 2) as avg_likes, 
ROUND(AVG(comments), 2) as avg_comments, 
ROUND(AVG(shares), 2) as avg_shares, 
ROUND(AVG(engagement_rate), 2) as avg_engagement, 
SUM(sentiment) as positive_posts, 
ROUND(SUM(sentiment) * 100.0 / COUNT(*), 2) as positive_percentage 
FROM sm_sentiment 
GROUP BY platform ORDER BY avg_engagement DESC;
```
Результат:

<img width="1397" height="356" alt="image" src="https://github.com/user-attachments/assets/1eede345-ab8d-4b54-bb0a-30df90b105bf" />

Можно увидеть, что Instagram лидирует по количеству постов и по среднему количеству лайков на пост, пользователи Facebook активнее обсуждают контент и чаще делятся им, на LinkedIn контент получает лучший отклик в расчете на охват, а пользователи Twitter чаще публикуют лояльный для аудитории контент (наибольшее количество позитивных откликов).

## Вывод
В ходе выполнения лабораторной работы был реализован полный цикл ETL-процесса для загрузки, обработки и анализа данных из CSV-файла в базу данных MySQL с использованием инструмента Pentaho. Все этапы обработки выполнены корректно, ошибки при экспорте данных были выявлены и устранены. Получилась готовая к анализу база данных.
